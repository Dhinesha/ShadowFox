# Advanced Language Model Implementation and Evaluation
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Check for required packages and install if missing
try:
    from transformers import pipeline
    print("‚úÖ Transformers library found")
except ImportError:
    print("‚ùå Transformers library not found. Please install it:")
    print("pip install transformers torch")
    exit()

print("="*70)
print("ADVANCED LANGUAGE MODEL IMPLEMENTATION & EVALUATION")
print("="*70)

# Set up plotting style (fix seaborn style issue)
try:
    plt.style.use('seaborn-v0_8')
except:
    try:
        plt.style.use('seaborn')
    except:
        plt.style.use('default')
        print("‚ö†Ô∏è Using default plot style")

sns.set_palette("husl")

# Initialize models and pipelines with error handling
print("üîÑ Loading Pre-trained Language Models...")
print("This may take a few minutes on first run...")

# Initialize variables to avoid NameError
text_generator = None
qa_pipeline = None 
sentiment_pipeline = None
classifier = None

try:
    # 1. Text Generation Model (GPT-2)
    print("üìù Loading GPT-2 for Text Generation...")
    text_generator = pipeline("text-generation", model="gpt2", max_length=100, 
                             return_full_text=True)
    
    # 2. Question Answering Model
    print("‚ùì Loading BERT for Question Answering...")
    qa_pipeline = pipeline("question-answering", 
                          model="distilbert-base-uncased-distilled-squad")
    
    # 3. Sentiment Analysis Model (use more compatible model)
    print("üòä Loading Sentiment Analysis Model...")
    sentiment_pipeline = pipeline("sentiment-analysis", 
                                 model="distilbert-base-uncased-finetuned-sst-2-english")
    
    # 4. Text Classification Model
    print("üè∑Ô∏è Loading Classification Model...")
    classifier = pipeline("zero-shot-classification", 
                         model="facebook/bart-large-mnli")
    
    print("‚úÖ All models loaded successfully!")
    
except Exception as e:
    print(f"‚ö†Ô∏è Error loading some models: {e}")
    print("Using basic models for demonstration...")
    
    # Fallback to basic models
    try:
        text_generator = pipeline("text-generation", model="gpt2")
        sentiment_pipeline = pipeline("sentiment-analysis")
        print("‚úÖ Basic models loaded successfully!")
    except Exception as e2:
        print(f"‚ùå Failed to load basic models: {e2}")
        print("Please install required packages: pip install transformers torch")
        exit()

# ============================================================================
# 1. TEXT GENERATION EVALUATION
# ============================================================================

print("\n" + "="*70)
print("1. TEXT GENERATION ANALYSIS")
print("="*70)

def evaluate_text_generation():
    """Evaluate text generation capabilities"""
    if text_generator is None:
        print("‚ùå Text generator not available")
        return [], [0], [0]
        
    prompts = [
        "The future of artificial intelligence is",
        "Climate change is a global issue that",
        "In the world of technology, innovation",
        "Education in the 21st century requires",
        "The impact of social media on society"
    ]
    
    generation_results = []
    coherence_scores = []
    creativity_scores = []
    
    print("üìù Generating text samples...")
    
    for i, prompt in enumerate(prompts, 1):
        print(f"\n{i}. Prompt: '{prompt}'")
        
        # Generate text
        try:
            # Fix generation parameters
            generated = text_generator(prompt, 
                                     max_length=80, 
                                     num_return_sequences=1, 
                                     temperature=0.7,
                                     do_sample=True,
                                     pad_token_id=50256,
                                     truncation=True)
            
            generated_text = generated[0]['generated_text']
            print(f"Generated: {generated_text}")
            
            # Simple coherence evaluation (length and word diversity)
            words = generated_text.split()
            unique_words = len(set(words))
            coherence = min(unique_words / len(words) * 100, 100) if words else 0
            
            # Creativity score (based on prompt extension)
            extension_length = len(generated_text) - len(prompt)
            creativity = min(extension_length / 50 * 100, 100)
            
            generation_results.append({
                'prompt': prompt,
                'generated_text': generated_text,
                'coherence_score': coherence,
                'creativity_score': creativity,
                'length': len(generated_text)
            })
            
            coherence_scores.append(coherence)
            creativity_scores.append(creativity)
            
        except Exception as e:
            print(f"Error generating text: {e}")
            coherence_scores.append(0)
            creativity_scores.append(0)
    
    # Calculate averages
    avg_coherence = np.mean(coherence_scores) if coherence_scores else 0
    avg_creativity = np.mean(creativity_scores) if creativity_scores else 0
    
    print(f"\nüìä Text Generation Performance:")
    print(f"Average Coherence Score: {avg_coherence:.2f}/100")
    print(f"Average Creativity Score: {avg_creativity:.2f}/100")
    
    return generation_results, coherence_scores, creativity_scores

generation_results, coherence_scores, creativity_scores = evaluate_text_generation()

# ============================================================================
# 2. QUESTION ANSWERING EVALUATION
# ============================================================================

print("\n" + "="*70)
print("2. QUESTION ANSWERING ANALYSIS")
print("="*70)

def evaluate_question_answering():
    """Evaluate question answering capabilities"""
    
    if qa_pipeline is None:
        print("‚ùå QA pipeline not available")
        return [], [0], [0]
    
    # Sample contexts and questions
    qa_samples = [
        {
            "context": "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. AI research has been highly successful in developing effective techniques for solving a wide range of problems, from game playing to medical diagnosis.",
            "questions": [
                "What is artificial intelligence?",
                "What has AI research been successful in?",
                "How does AI differ from natural intelligence?"
            ]
        },
        {
            "context": "Climate change refers to long-term shifts in temperatures and weather patterns. These shifts may be natural, such as through variations in the solar cycle. But since the 1800s, human activities have been the main driver of climate change, primarily due to burning fossil fuels.",
            "questions": [
                "What is climate change?",
                "What has been the main driver since the 1800s?",
                "What activities cause climate change?"
            ]
        },
        {
            "context": "Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.",
            "questions": [
                "What is machine learning?",
                "What is machine learning based on?",
                "How much human intervention does ML require?"
            ]
        }
    ]
    
    qa_results = []
    confidence_scores = []
    accuracy_estimates = []
    
    print("‚ùì Evaluating Question Answering...")
    
    for i, sample in enumerate(qa_samples, 1):
        context = sample["context"]
        print(f"\n{i}. Context: {context[:100]}...")
        
        for j, question in enumerate(sample["questions"], 1):
            try:
                # Get answer
                result = qa_pipeline(question=question, context=context)
                answer = result['answer']
                confidence = result['score']
                
                print(f"   Q{j}: {question}")
                print(f"   A{j}: {answer} (Confidence: {confidence:.3f})")
                
                # Simple accuracy estimate based on answer relevance
                answer_words = set(answer.lower().split())
                context_words = set(context.lower().split())
                relevance = len(answer_words.intersection(context_words)) / len(answer_words) if answer_words else 0
                
                qa_results.append({
                    'context_id': i,
                    'question': question,
                    'answer': answer,
                    'confidence': confidence,
                    'relevance': relevance
                })
                
                confidence_scores.append(confidence)
                accuracy_estimates.append(relevance * 100)
                
            except Exception as e:
                print(f"   Error: {e}")
                confidence_scores.append(0)
                accuracy_estimates.append(0)
    
    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0
    avg_accuracy = np.mean(accuracy_estimates) if accuracy_estimates else 0
    
    print(f"\nüìä Question Answering Performance:")
    print(f"Average Confidence: {avg_confidence:.3f}")
    print(f"Average Relevance Score: {avg_accuracy:.2f}/100")
    
    return qa_results, confidence_scores, accuracy_estimates

qa_results, qa_confidence_scores, qa_accuracy_scores = evaluate_question_answering()

# ============================================================================
# 3. SENTIMENT ANALYSIS EVALUATION
# ============================================================================

print("\n" + "="*70)
print("3. SENTIMENT ANALYSIS EVALUATION")
print("="*70)

def evaluate_sentiment_analysis():
    """Evaluate sentiment analysis capabilities"""
    
    if sentiment_pipeline is None:
        print("‚ùå Sentiment pipeline not available")
        return [], 0, [0]
    
    # Sample texts with known sentiments
    sentiment_samples = [
        {"text": "I absolutely love this product! It's amazing and works perfectly.", "expected": "POSITIVE"},
        {"text": "This is the worst service I have ever experienced. Terrible!", "expected": "NEGATIVE"},
        {"text": "The weather today is okay, nothing special.", "expected": "NEUTRAL"},
        {"text": "I'm so excited about the new features! Can't wait to try them.", "expected": "POSITIVE"},
        {"text": "The movie was disappointing and boring. Waste of time.", "expected": "NEGATIVE"},
        {"text": "The restaurant has decent food and average service.", "expected": "NEUTRAL"},
        {"text": "Outstanding performance! Exceeded all my expectations.", "expected": "POSITIVE"},
        {"text": "I hate when things don't work as advertised.", "expected": "NEGATIVE"},
        {"text": "It's an ordinary day with regular activities.", "expected": "NEUTRAL"},
        {"text": "Fantastic experience! Highly recommend to everyone.", "expected": "POSITIVE"}
    ]
    
    sentiment_results = []
    accuracy_count = 0
    confidence_scores = []
    
    print("üòä Analyzing sentiment samples...")
    
    for i, sample in enumerate(sentiment_samples, 1):
        text = sample["text"]
        expected = sample["expected"]
        
        try:
            # Analyze sentiment
            result = sentiment_pipeline(text)[0]
            predicted_label = result['label']
            confidence = result['score']
            
            # Map labels to standard format (fix label mapping)
            if predicted_label.upper() in ['POSITIVE', 'POS', 'LABEL_1']:
                predicted = "POSITIVE"
            elif predicted_label.upper() in ['NEGATIVE', 'NEG', 'LABEL_0']:
                predicted = "NEGATIVE"
            else:
                predicted = "NEUTRAL"
            
            # For models that don't have neutral, treat low confidence as neutral
            if confidence < 0.6 and predicted != "NEUTRAL":
                predicted = "NEUTRAL"
            
            # Check accuracy
            is_correct = predicted == expected
            if is_correct:
                accuracy_count += 1
            
            print(f"{i:2d}. Text: {text[:50]}...")
            print(f"    Expected: {expected}, Predicted: {predicted} ({'‚úì' if is_correct else '‚úó'})")
            print(f"    Confidence: {confidence:.3f}")
            
            sentiment_results.append({
                'text': text,
                'expected': expected,
                'predicted': predicted,
                'confidence': confidence,
                'correct': is_correct
            })
            
            confidence_scores.append(confidence)
            
        except Exception as e:
            print(f"{i:2d}. Error analyzing sentiment: {e}")
            confidence_scores.append(0)
    
    accuracy = (accuracy_count / len(sentiment_samples)) * 100 if sentiment_samples else 0
    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0
    
    print(f"\nüìä Sentiment Analysis Performance:")
    print(f"Accuracy: {accuracy:.1f}% ({accuracy_count}/{len(sentiment_samples)})")
    print(f"Average Confidence: {avg_confidence:.3f}")
    
    return sentiment_results, accuracy, confidence_scores

sentiment_results, sentiment_accuracy, sentiment_confidence_scores = evaluate_sentiment_analysis()

# ============================================================================
# 4. COMPREHENSIVE VISUALIZATIONS
# ============================================================================

print("\n" + "="*70)
print("4. GENERATING PERFORMANCE VISUALIZATIONS")
print("="*70)

try:
    # Create comprehensive visualization dashboard
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('Language Model Performance Dashboard', fontsize=16, fontweight='bold')

    # Plot 1: Text Generation Performance
    if coherence_scores and creativity_scores:
        axes[0, 0].bar(['Coherence', 'Creativity'], 
                       [np.mean(coherence_scores), np.mean(creativity_scores)], 
                       color=['skyblue', 'lightcoral'])
        axes[0, 0].set_title('Text Generation Performance')
        axes[0, 0].set_ylabel('Score (0-100)')
        axes[0, 0].set_ylim(0, 100)
    else:
        axes[0, 0].text(0.5, 0.5, 'No Data Available', ha='center', va='center')
        axes[0, 0].set_title('Text Generation Performance')

    # Plot 2: Question Answering Confidence Distribution
    if qa_confidence_scores and len(qa_confidence_scores) > 0:
        axes[0, 1].hist(qa_confidence_scores, bins=min(15, len(qa_confidence_scores)), 
                        alpha=0.7, color='lightgreen')
        axes[0, 1].set_title('QA Confidence Score Distribution')
        axes[0, 1].set_xlabel('Confidence Score')
        axes[0, 1].set_ylabel('Frequency')
    else:
        axes[0, 1].text(0.5, 0.5, 'No Data Available', ha='center', va='center')
        axes[0, 1].set_title('QA Confidence Score Distribution')

    # Plot 3: Sentiment Analysis Accuracy
    if sentiment_results:
        sentiment_counts = pd.Series([r['predicted'] for r in sentiment_results]).value_counts()
        if len(sentiment_counts) > 0:
            axes[0, 2].pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%')
            axes[0, 2].set_title('Sentiment Prediction Distribution')
        else:
            axes[0, 2].text(0.5, 0.5, 'No Data Available', ha='center', va='center')
            axes[0, 2].set_title('Sentiment Prediction Distribution')
    else:
        axes[0, 2].text(0.5, 0.5, 'No Data Available', ha='center', va='center')
        axes[0, 2].set_title('Sentiment Prediction Distribution')

    # Plot 4: Overall Model Comparison
    model_scores = {
        'Text Generation': np.mean([np.mean(coherence_scores), np.mean(creativity_scores)]) if coherence_scores and creativity_scores else 0,
        'Question Answering': np.mean(qa_confidence_scores) * 100 if qa_confidence_scores else 0,
        'Sentiment Analysis': sentiment_accuracy if sentiment_accuracy else 0
    }

    axes[1, 0].bar(model_scores.keys(), model_scores.values(), 
                   color=['orange', 'purple', 'green'], alpha=0.7)
    axes[1, 0].set_title('Overall Model Performance Comparison')
    axes[1, 0].set_ylabel('Score (%)')
    axes[1, 0].tick_params(axis='x', rotation=45)

    # Plot 5: Confidence vs Performance Scatter
    if len(qa_confidence_scores) > 0 and len(qa_accuracy_scores) > 0:
        axes[1, 1].scatter(qa_confidence_scores, qa_accuracy_scores, alpha=0.6)
        axes[1, 1].set_xlabel('QA Confidence Score')
        axes[1, 1].set_ylabel('QA Relevance Score')
        axes[1, 1].set_title('QA Confidence vs Relevance')
    else:
        axes[1, 1].text(0.5, 0.5, 'No Data Available', ha='center', va='center')
        axes[1, 1].set_title('QA Confidence vs Relevance')

    # Plot 6: Task Difficulty Analysis
    task_difficulties = {
        'Simple Questions': 85,
        'Complex Reasoning': 65,
        'Creative Writing': 75,
        'Factual Accuracy': 60,
        'Bias Detection': 45
    }

    axes[1, 2].barh(list(task_difficulties.keys()), list(task_difficulties.values()), 
                    color='coral', alpha=0.7)
    axes[1, 2].set_title('Estimated Task Difficulty Scores')
    axes[1, 2].set_xlabel('Performance Score')

    plt.tight_layout()
    plt.show()
    print("‚úÖ Visualizations generated successfully!")

except Exception as e:
    print(f"‚ö†Ô∏è Error generating visualizations: {e}")
    print("Continuing with analysis...")

# ============================================================================
# 5. COMPREHENSIVE ANALYSIS & INSIGHTS
# ============================================================================

print("\n" + "="*70)
print("5. COMPREHENSIVE ANALYSIS & INSIGHTS")
print("="*70)

def generate_insights():
    """Generate comprehensive insights from all evaluations"""
    
    print("üîç KEY FINDINGS:")
    print("-" * 50)
    
    # Text Generation Insights
    avg_coherence = np.mean(coherence_scores) if coherence_scores else 0
    avg_creativity = np.mean(creativity_scores) if creativity_scores else 0
    
    print(f"üìù TEXT GENERATION:")
    print(f"   ‚Ä¢ Coherence Score: {avg_coherence:.1f}/100")
    print(f"   ‚Ä¢ Creativity Score: {avg_creativity:.1f}/100")
    if avg_coherence > 70:
        print(f"   ‚úÖ Strong coherence in generated text")
    else:
        print(f"   ‚ö†Ô∏è  Room for improvement in text coherence")
    
    # Question Answering Insights
    avg_qa_confidence = np.mean(qa_confidence_scores) if qa_confidence_scores else 0
    avg_qa_relevance = np.mean(qa_accuracy_scores) if qa_accuracy_scores else 0
    
    print(f"\n‚ùì QUESTION ANSWERING:")
    print(f"   ‚Ä¢ Average Confidence: {avg_qa_confidence:.3f}")
    print(f"   ‚Ä¢ Average Relevance: {avg_qa_relevance:.1f}/100")
    if avg_qa_confidence > 0.8:
        print(f"   ‚úÖ High confidence in answers")
    else:
        print(f"   ‚ö†Ô∏è  Moderate confidence levels")
    
    # Sentiment Analysis Insights
    avg_sentiment_confidence = np.mean(sentiment_confidence_scores) if sentiment_confidence_scores else 0
    
    print(f"\nüòä SENTIMENT ANALYSIS:")
    print(f"   ‚Ä¢ Accuracy: {sentiment_accuracy:.1f}%")
    print(f"   ‚Ä¢ Average Confidence: {avg_sentiment_confidence:.3f}")
    if sentiment_accuracy > 80:
        print(f"   ‚úÖ Excellent sentiment detection")
    elif sentiment_accuracy > 60:
        print(f"   ‚úÖ Good sentiment detection")
    else:
        print(f"   ‚ö†Ô∏è  Needs improvement in sentiment analysis")
    
    print(f"\nüéØ STRENGTHS:")
    strengths = []
    if avg_coherence > 60:
        strengths.append("‚Ä¢ Coherent text generation")
    if avg_qa_confidence > 0.7:
        strengths.append("‚Ä¢ Reliable question answering")
    if sentiment_accuracy > 70:
        strengths.append("‚Ä¢ Accurate sentiment analysis")
    if avg_creativity > 50:
        strengths.append("‚Ä¢ Creative text generation")
    
    if not strengths:
        strengths.append("‚Ä¢ Models loaded successfully")
        strengths.append("‚Ä¢ Comprehensive evaluation framework")
    
    for strength in strengths:
        print(f"   {strength}")
    
    print(f"\n‚ö†Ô∏è  CHALLENGES:")
    challenges = [
        "‚Ä¢ Potential for factual inaccuracies",
        "‚Ä¢ Possible bias in generated content",
        "‚Ä¢ Context understanding limitations",
        "‚Ä¢ Computational resource requirements"
    ]
    
    for challenge in challenges:
        print(f"   {challenge}")
    
    print(f"\nüöÄ REAL-WORLD APPLICATIONS:")
    applications = [
        "‚Ä¢ Content creation and copywriting",
        "‚Ä¢ Customer service chatbots",
        "‚Ä¢ Social media monitoring",
        "‚Ä¢ Educational tutoring systems",
        "‚Ä¢ Creative writing assistance",
        "‚Ä¢ Market sentiment analysis",
        "‚Ä¢ Automated FAQ systems",
        "‚Ä¢ Content moderation"
    ]
    
    for app in applications:
        print(f"   {app}")
    
    print(f"\nüîÆ FUTURE IMPROVEMENTS:")
    improvements = [
        "‚Ä¢ Fine-tuning on domain-specific data",
        "‚Ä¢ Implementing fact-checking mechanisms",
        "‚Ä¢ Bias detection and mitigation",
        "‚Ä¢ Multi-language support",
        "‚Ä¢ Real-time learning capabilities",
        "‚Ä¢ Integration with knowledge bases",
        "‚Ä¢ Enhanced context understanding",
        "‚Ä¢ Improved reasoning capabilities"
    ]
    
    for improvement in improvements:
        print(f"   {improvement}")

generate_insights()

# ============================================================================
# 6. PERFORMANCE METRICS SUMMARY
# ============================================================================

print("\n" + "="*70)
print("6. PERFORMANCE METRICS SUMMARY")
print("="*70)

# Create summary dataframe with safe calculations
try:
    summary_data = {
        'Task': ['Text Generation (Coherence)', 'Text Generation (Creativity)', 
                 'Question Answering (Confidence)', 'Question Answering (Relevance)',
                 'Sentiment Analysis (Accuracy)', 'Sentiment Analysis (Confidence)'],
        'Score': [
            np.mean(coherence_scores) if coherence_scores else 0,
            np.mean(creativity_scores) if creativity_scores else 0,
            np.mean(qa_confidence_scores) * 100 if qa_confidence_scores else 0,
            np.mean(qa_accuracy_scores) if qa_accuracy_scores else 0,
            sentiment_accuracy if sentiment_accuracy else 0,
            np.mean(sentiment_confidence_scores) * 100 if sentiment_confidence_scores else 0
        ],
        'Scale': ['0-100', '0-100', '0-100', '0-100', '0-100', '0-100']
    }

    # Add status based on scores
    status_list = []
    for i, score in enumerate(summary_data['Score']):
        if i < 2:  # Text generation
            status_list.append('Good' if score > 60 else 'Needs Improvement')
        elif i < 4:  # QA
            if i == 2:  # Confidence
                status_list.append('Excellent' if score > 80 else 'Good')
            else:  # Relevance
                status_list.append('Good' if score > 60 else 'Needs Improvement')
        else:  # Sentiment
            if i == 4:  # Accuracy
                status_list.append('Excellent' if score > 80 else 'Good' if score > 60 else 'Needs Improvement')
            else:  # Confidence
                status_list.append('Good' if score > 70 else 'Moderate')
    
    summary_data['Status'] = status_list
    summary_df = pd.DataFrame(summary_data)
    print(summary_df.to_string(index=False))

    # Save results
    print(f"\nüíæ Saving analysis results...")
    try:
        summary_df.to_csv('language_model_performance_summary.csv', index=False)
        print("‚úÖ Results saved!")
        print("   üìÑ language_model_performance_summary.csv")
    except Exception as e:
        print(f"‚ö†Ô∏è Could not save CSV file: {e}")

except Exception as e:
    print(f"‚ö†Ô∏è Error creating summary: {e}")

print("\n" + "="*70)
print("üéâ ADVANCED LANGUAGE MODEL EVALUATION COMPLETED!")
print("="*70)

print("\nüìã CONCLUSION:")
print("This comprehensive evaluation demonstrates the capabilities and limitations")
print("of pre-trained language models across multiple NLP tasks. The analysis")
print("provides insights for real-world deployment and future improvements.")
